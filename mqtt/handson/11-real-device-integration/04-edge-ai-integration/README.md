# æ¼”ç¿’4: ã‚¨ãƒƒã‚¸AIçµ±åˆã¨MQTTé€£æº

## æ¦‚è¦

NVIDIA Jetsonã®GPUèƒ½åŠ›ã‚’æ´»ç”¨ã—ã¦ã€ã‚¨ãƒƒã‚¸ã§ã®AIæ¨è«–ã¨MQTTé€šä¿¡ã‚’çµ„ã¿åˆã‚ã›ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒèªè­˜ã€éŸ³å£°å‡¦ç†ã€ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’ã‚¨ãƒƒã‚¸ã§å®Ÿè¡Œã—ã€çµæœã‚’MQTTçµŒç”±ã§ã‚¯ãƒ©ã‚¦ãƒ‰ã«é€ä¿¡ã—ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

- Jetsonã§ã®TensorRTæœ€é©åŒ–AIæ¨è«–
- ã‚«ãƒ¡ãƒ©ãƒ»éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
- AIæ¨è«–çµæœã®MQTTé€ä¿¡
- ã‚¨ãƒƒã‚¸-ã‚¯ãƒ©ã‚¦ãƒ‰é€£æºã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- è¤‡æ•°AI ãƒ¢ãƒ‡ãƒ«ã®ä¸¦åˆ—å®Ÿè¡Œ

## å¿…è¦æ©Ÿæ

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢
- NVIDIA Jetson Xavier NX/Orin Nano (Nanoã§ã‚‚å¯ã€ãŸã ã—æ€§èƒ½åˆ¶é™ã‚ã‚Š)
- CSIã‚«ãƒ¡ãƒ© (Raspberry Pi Camera V2æ¨å¥¨) ã¾ãŸã¯ USBã‚«ãƒ¡ãƒ©
- USB ãƒã‚¤ã‚¯
- microSDã‚«ãƒ¼ãƒ‰ (64GBä»¥ä¸Šæ¨å¥¨)
- å†·å´ãƒ•ã‚¡ãƒ³ï¼ˆé•·æ™‚é–“AIå‡¦ç†æ™‚ï¼‰

### ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç’°å¢ƒ
- JetPack SDK 5.1.2+
- TensorRT 8.5+
- OpenCV 4.8+
- PyTorch 2.0+ (JetPackç‰ˆ)

## æ¼”ç¿’æ‰‹é †

### Step 1: AIç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 1.1 JetPack AI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç¢ºèª

```bash
# JetPack ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç¢ºèª
jetson_release -v

# CUDAç¢ºèª
nvcc --version
nvidia-smi  # Orinç³»ã®ã¿

# TensorRTç¢ºèª
python3 -c "import tensorrt; print(f'TensorRT version: {tensorrt.__version__}')"

# PyTorchç¢ºèª
python3 -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

#### 1.2 AIé–‹ç™ºç’°å¢ƒæ§‹ç¯‰

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python3 -m venv ~/jetson_ai_env
source ~/jetson_ai_env/bin/activate

# JetPackå¯¾å¿œãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# OpenCVç¢ºèªï¼ˆCUDAå¯¾å¿œç‰ˆï¼‰
python3 -c "import cv2; print(f'OpenCV: {cv2.__version__}, CUDA: {cv2.cuda.getCudaEnabledDeviceCount()}')"
```

### Step 2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒèªè­˜ã‚·ã‚¹ãƒ†ãƒ 

#### 2.1 YOLOç‰©ä½“æ¤œå‡º + MQTT

`src/yolo_mqtt_detector.py`:

```python
#!/usr/bin/env python3
import cv2
import numpy as np
import torch
import torchvision
import json
import time
import asyncio
import threading
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import paho.mqtt.client as mqtt

# Jetsonã§ã®TensorRTæœ€é©åŒ–
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    print("âš  TensorRT not available, using PyTorch inference")
    TENSORRT_AVAILABLE = False

class YOLODetector:
    """YOLOç‰©ä½“æ¤œå‡ºå™¨ï¼ˆTensorRTæœ€é©åŒ–ï¼‰"""
    
    def __init__(self, model_name: str = 'yolov5s', confidence_threshold: float = 0.5):
        self.model_name = model_name
        self.confidence_threshold = confidence_threshold
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.model = self.load_model()
        self.class_names = self.model.names if hasattr(self.model, 'names') else []
        
        print(f"âœ“ YOLO model loaded: {model_name} on {self.device}")
    
    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            # YOLOv5 Hub ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model = torch.hub.load('ultralytics/yolov5', self.model_name, pretrained=True)
            model.to(self.device)
            model.eval()
            
            # TensorRTæœ€é©åŒ–ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            if TENSORRT_AVAILABLE and self.device.type == 'cuda':
                model = self.optimize_tensorrt(model)
            
            return model
            
        except Exception as e:
            print(f"Model loading error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šäº‹å‰è¨“ç·´æ¸ˆã¿ResNet
            model = torchvision.models.resnet18(pretrained=True)
            model.to(self.device)
            model.eval()
            return model
    
    def optimize_tensorrt(self, model):
        """TensorRTæœ€é©åŒ–"""
        try:
            # TorchScriptå¤‰æ›
            model.eval()
            with torch.no_grad():
                traced_model = torch.jit.trace(model, torch.randn(1, 3, 640, 640).to(self.device))
            
            # TensorRTæœ€é©åŒ–
            from torch2trt import torch2trt
            model_trt = torch2trt(traced_model, [torch.randn(1, 3, 640, 640).to(self.device)],
                                fp16_mode=True, max_workspace_size=1<<30)
            
            print("âœ“ TensorRT optimization successful")
            return model_trt
            
        except Exception as e:
            print(f"TensorRT optimization failed: {e}")
            return model
    
    def detect(self, frame: np.ndarray) -> List[Dict]:
        """ç‰©ä½“æ¤œå‡ºå®Ÿè¡Œ"""
        try:
            # å‰å‡¦ç†
            input_tensor = self.preprocess_frame(frame)
            
            # æ¨è«–å®Ÿè¡Œ
            with torch.no_grad():
                start_time = time.time()
                results = self.model(input_tensor)
                inference_time = (time.time() - start_time) * 1000  # ms
            
            # å¾Œå‡¦ç†
            detections = self.postprocess_results(results, frame.shape, inference_time)
            
            return detections
            
        except Exception as e:
            print(f"Detection error: {e}")
            return []
    
    def preprocess_frame(self, frame: np.ndarray) -> torch.Tensor:
        """ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†"""
        # YOLOv5å…¥åŠ›ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
        resized = cv2.resize(frame, (640, 640))
        
        # BGR â†’ RGB
        rgb_frame = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # æ­£è¦åŒ–ã¨ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
        tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).float() / 255.0
        tensor = tensor.unsqueeze(0).to(self.device)
        
        return tensor
    
    def postprocess_results(self, results, original_shape: Tuple, inference_time: float) -> List[Dict]:
        """çµæœå¾Œå‡¦ç†"""
        detections = []
        
        try:
            # YOLOv5çµæœè§£æ
            if hasattr(results, 'xyxy'):
                predictions = results.xyxy[0].cpu().numpy()
            else:
                predictions = results[0].cpu().numpy() if isinstance(results, (list, tuple)) else results.cpu().numpy()
            
            h_orig, w_orig = original_shape[:2]
            h_input, w_input = 640, 640
            
            for pred in predictions:
                confidence = pred[4] if len(pred) > 4 else 1.0
                
                if confidence >= self.confidence_threshold:
                    # åº§æ¨™ã‚’ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«
                    x1, y1, x2, y2 = pred[:4]
                    x1 = int(x1 * w_orig / w_input)
                    y1 = int(y1 * h_orig / h_input)
                    x2 = int(x2 * w_orig / w_input)
                    y2 = int(y2 * h_orig / h_input)
                    
                    class_id = int(pred[5]) if len(pred) > 5 else 0
                    class_name = self.class_names[class_id] if class_id < len(self.class_names) else f'class_{class_id}'
                    
                    detection = {
                        'bbox': [x1, y1, x2, y2],
                        'confidence': float(confidence),
                        'class_id': class_id,
                        'class_name': class_name,
                        'area': (x2 - x1) * (y2 - y1)
                    }
                    
                    detections.append(detection)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            if detections:
                detections[0]['inference_time_ms'] = round(inference_time, 2)
                detections[0]['total_objects'] = len(detections)
            
        except Exception as e:
            print(f"Postprocessing error: {e}")
        
        return detections

class CameraCapture:
    """ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ç®¡ç†"""
    
    def __init__(self, camera_id: int = 0, width: int = 1280, height: int = 720, fps: int = 30):
        self.camera_id = camera_id
        self.width = width
        self.height = height
        self.fps = fps
        self.cap = None
        self.frame_count = 0
        
    def initialize(self) -> bool:
        """ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–"""
        try:
            # CSIã‚«ãƒ¡ãƒ©ï¼ˆJetsonï¼‰ã®å ´åˆ
            if self.camera_id == 0:
                gstreamer_pipeline = (
                    f"nvarguscamerasrc sensor-id=0 ! "
                    f"video/x-raw(memory:NVMM), width={self.width}, height={self.height}, "
                    f"format=NV12, framerate={self.fps}/1 ! "
                    f"nvvidconv ! video/x-raw, format=BGRx ! "
                    f"videoconvert ! video/x-raw, format=BGR ! appsink"
                )
                
                self.cap = cv2.VideoCapture(gstreamer_pipeline, cv2.CAP_GSTREAMER)
            else:
                # USBã‚«ãƒ¡ãƒ©
                self.cap = cv2.VideoCapture(self.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.fps)
            
            if not self.cap.isOpened():
                raise Exception("Failed to open camera")
            
            print(f"âœ“ Camera initialized: {self.width}x{self.height}@{self.fps}fps")
            return True
            
        except Exception as e:
            print(f"Camera initialization error: {e}")
            return False
    
    def read_frame(self) -> Optional[np.ndarray]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿å–ã‚Š"""
        if self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if ret:
                self.frame_count += 1
                return frame
        return None
    
    def release(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        if self.cap:
            self.cap.release()

class EdgeAIMQTTClient:
    """ã‚¨ãƒƒã‚¸AI + MQTTã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, broker_host: str = "localhost", device_id: str = None):
        self.broker_host = broker_host
        self.device_id = device_id or f"jetson_ai_{int(time.time())}"
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.detector = YOLODetector()
        self.camera = CameraCapture()
        self.mqtt_client = mqtt.Client(client_id=self.device_id)
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'frames_processed': 0,
            'detections_sent': 0,
            'average_fps': 0.0,
            'average_inference_time': 0.0,
            'start_time': time.time()
        }
        
        # MQTTè¨­å®š
        self.setup_mqtt()
        
    def setup_mqtt(self):
        """MQTTè¨­å®š"""
        self.mqtt_client.on_connect = self.on_connect
        self.mqtt_client.on_disconnect = self.on_disconnect
        self.mqtt_client.on_message = self.on_message
    
    def on_connect(self, client, userdata, flags, rc):
        if rc == 0:
            print(f"âœ“ MQTT connected: {self.device_id}")
            client.subscribe(f"ai/{self.device_id}/command", qos=1)
        else:
            print(f"âœ— MQTT connection failed: {rc}")
    
    def on_disconnect(self, client, userdata, rc):
        if rc != 0:
            print(f"âš  Unexpected MQTT disconnect: {rc}")
    
    def on_message(self, client, userdata, msg):
        """ã‚³ãƒãƒ³ãƒ‰å—ä¿¡å‡¦ç†"""
        try:
            command = json.loads(msg.payload.decode())
            asyncio.create_task(self.handle_command(command))
        except Exception as e:
            print(f"Command processing error: {e}")
    
    async def handle_command(self, command: Dict):
        """ã‚³ãƒãƒ³ãƒ‰å‡¦ç†"""
        cmd_type = command.get('type')
        
        if cmd_type == 'status':
            self.publish_status()
        elif cmd_type == 'config':
            self.update_config(command.get('config', {}))
        elif cmd_type == 'capture':
            await self.capture_and_analyze()
    
    def publish_detections(self, detections: List[Dict], frame_info: Dict):
        """æ¤œå‡ºçµæœé€ä¿¡"""
        if not detections:
            return
        
        detection_data = {
            'timestamp': datetime.now().isoformat(),
            'device_id': self.device_id,
            'frame_info': frame_info,
            'detections': detections,
            'summary': {
                'object_count': len(detections),
                'classes': list(set(d['class_name'] for d in detections)),
                'avg_confidence': sum(d['confidence'] for d in detections) / len(detections)
            }
        }
        
        topic = f"ai/{self.device_id}/detections"
        self.mqtt_client.publish(topic, json.dumps(detection_data), qos=1)
        self.stats['detections_sent'] += 1
    
    def publish_status(self):
        """çŠ¶æ…‹é€ä¿¡"""
        uptime = time.time() - self.stats['start_time']
        
        status_data = {
            'timestamp': datetime.now().isoformat(),
            'device_id': self.device_id,
            'status': 'running',
            'uptime_seconds': round(uptime, 2),
            'statistics': {
                'frames_processed': self.stats['frames_processed'],
                'detections_sent': self.stats['detections_sent'],
                'average_fps': self.stats['average_fps'],
                'average_inference_time_ms': self.stats['average_inference_time']
            },
            'hardware': {
                'gpu_available': torch.cuda.is_available(),
                'gpu_memory_mb': torch.cuda.get_device_properties(0).total_memory // 1024 // 1024 if torch.cuda.is_available() else 0,
                'tensorrt_enabled': TENSORRT_AVAILABLE
            }
        }
        
        topic = f"ai/{self.device_id}/status"
        self.mqtt_client.publish(topic, json.dumps(status_data), qos=1, retain=True)
    
    async def capture_and_analyze(self):
        """å˜ç™ºã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ»è§£æ"""
        frame = self.camera.read_frame()
        if frame is not None:
            detections = self.detector.detect(frame)
            
            frame_info = {
                'width': frame.shape[1],
                'height': frame.shape[0],
                'channels': frame.shape[2],
                'frame_number': self.camera.frame_count
            }
            
            self.publish_detections(detections, frame_info)
    
    async def run_continuous_detection(self):
        """é€£ç¶šæ¤œå‡ºãƒ«ãƒ¼ãƒ—"""
        print("ğŸš€ Starting Edge AI MQTT Client")
        
        # åˆæœŸåŒ–
        if not self.camera.initialize():
            print("âŒ Camera initialization failed")
            return
        
        try:
            self.mqtt_client.connect(self.broker_host, 1883, 60)
            self.mqtt_client.loop_start()
            
            # FPSè¨ˆç®—ç”¨
            fps_counter = 0
            fps_start_time = time.time()
            inference_times = []
            
            while True:
                frame = self.camera.read_frame()
                if frame is None:
                    continue
                
                # AIæ¨è«–å®Ÿè¡Œ
                detections = self.detector.detect(frame)
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±
                frame_info = {
                    'width': frame.shape[1],
                    'height': frame.shape[0],
                    'channels': frame.shape[2],
                    'frame_number': self.camera.frame_count,
                    'timestamp': datetime.now().isoformat()
                }
                
                # æ¤œå‡ºçµæœé€ä¿¡ï¼ˆæ¤œå‡ºãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
                if detections:
                    self.publish_detections(detections, frame_info)
                    
                    # æ¨è«–æ™‚é–“çµ±è¨ˆ
                    if detections and 'inference_time_ms' in detections[0]:
                        inference_times.append(detections[0]['inference_time_ms'])
                        if len(inference_times) > 100:
                            inference_times.pop(0)
                
                # çµ±è¨ˆæ›´æ–°
                self.stats['frames_processed'] += 1
                fps_counter += 1
                
                # FPSè¨ˆç®—ï¼ˆ5ç§’é–“éš”ï¼‰
                if time.time() - fps_start_time >= 5.0:
                    self.stats['average_fps'] = fps_counter / 5.0
                    fps_counter = 0
                    fps_start_time = time.time()
                    
                    # æ¨è«–æ™‚é–“å¹³å‡
                    if inference_times:
                        self.stats['average_inference_time'] = sum(inference_times) / len(inference_times)
                    
                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡
                    self.publish_status()
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                await asyncio.sleep(0.033)  # ~30 FPS
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping Edge AI client...")
        except Exception as e:
            print(f"Runtime error: {e}")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.camera.release()
        self.mqtt_client.loop_stop()
        self.mqtt_client.disconnect()
        print("âœ“ Edge AI client stopped")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import os
    
    broker_host = os.getenv('MQTT_BROKER_HOST', 'localhost')
    device_id = os.getenv('DEVICE_ID', None)
    
    client = EdgeAIMQTTClient(broker_host, device_id)
    
    try:
        asyncio.run(client.run_continuous_detection())
    except KeyboardInterrupt:
        print("\nShutting down...")

if __name__ == "__main__":
    main()
```

#### 2.2 éŸ³å£°èªè­˜çµ±åˆ

`src/audio_recognition.py`:

```python
#!/usr/bin/env python3
import numpy as np
import pyaudio
import wave
import json
import time
import asyncio
import threading
from datetime import datetime
import paho.mqtt.client as mqtt

# éŸ³å£°èªè­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import speech_recognition as sr
    import webrtcvad
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    print("âš  Speech recognition not available")
    SPEECH_RECOGNITION_AVAILABLE = False

class AudioProcessor:
    """éŸ³å£°å‡¦ç†ãƒ»èªè­˜"""
    
    def __init__(self, sample_rate: int = 16000, chunk_size: int = 1024):
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        
        # PyAudioåˆæœŸåŒ–
        self.audio = pyaudio.PyAudio()
        self.stream = None
        
        # éŸ³å£°èªè­˜åˆæœŸåŒ–
        if SPEECH_RECOGNITION_AVAILABLE:
            self.recognizer = sr.Recognizer()
            self.vad = webrtcvad.Vad(2)  # Aggressiveness level (0-3)
        
        # éŸ³å£°ãƒãƒƒãƒ•ã‚¡
        self.audio_buffer = []
        self.is_recording = False
        
        print("ğŸ¤ Audio processor initialized")
    
    def start_audio_stream(self):
        """éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹"""
        try:
            self.stream = self.audio.open(
                format=self.audio_format,
                channels=self.channels,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size,
                stream_callback=self.audio_callback
            )
            self.stream.start_stream()
            print("âœ“ Audio stream started")
            return True
            
        except Exception as e:
            print(f"Audio stream error: {e}")
            return False
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³å£°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        if self.is_recording:
            self.audio_buffer.append(in_data)
        
        # éŸ³å£°æ´»æ€§æ¤œå‡ºï¼ˆVADï¼‰
        if SPEECH_RECOGNITION_AVAILABLE:
            audio_frame = np.frombuffer(in_data, dtype=np.int16)
            if self.detect_speech(audio_frame):
                if not self.is_recording:
                    self.start_recording()
            else:
                if self.is_recording and len(self.audio_buffer) > 10:  # æœ€å°éŒ²éŸ³é•·
                    self.stop_recording()
        
        return (in_data, pyaudio.paContinue)
    
    def detect_speech(self, audio_frame: np.ndarray) -> bool:
        """éŸ³å£°æ¤œå‡º"""
        if not SPEECH_RECOGNITION_AVAILABLE:
            return False
        
        try:
            # WebRTC VADã¯10ms, 20ms, 30msãƒ•ãƒ¬ãƒ¼ãƒ ãŒå¿…è¦
            frame_duration_ms = len(audio_frame) * 1000 // self.sample_rate
            
            if frame_duration_ms in [10, 20, 30]:
                # 16bitã‚µãƒ³ãƒ—ãƒ«ã‚’8bitã«å¤‰æ›
                audio_bytes = audio_frame.tobytes()
                return self.vad.is_speech(audio_bytes, self.sample_rate)
        except:
            pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: éŸ³é‡ãƒ™ãƒ¼ã‚¹æ¤œå‡º
        rms = np.sqrt(np.mean(audio_frame**2))
        return rms > 1000  # é–¾å€¤èª¿æ•´ãŒå¿…è¦
    
    def start_recording(self):
        """éŒ²éŸ³é–‹å§‹"""
        if not self.is_recording:
            self.is_recording = True
            self.audio_buffer = []
            print("ğŸ”´ Recording started")
    
    def stop_recording(self):
        """éŒ²éŸ³çµ‚äº†ãƒ»éŸ³å£°èªè­˜å®Ÿè¡Œ"""
        if self.is_recording:
            self.is_recording = False
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_data = b''.join(self.audio_buffer)
            
            # éŸ³å£°èªè­˜ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            threading.Thread(target=self.process_audio, args=(audio_data,), daemon=True).start()
            
            print("â¹ï¸ Recording stopped, processing...")
    
    def process_audio(self, audio_data: bytes):
        """éŸ³å£°èªè­˜å‡¦ç†"""
        if not SPEECH_RECOGNITION_AVAILABLE:
            return
        
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’AudioDataã«å¤‰æ›
            audio_np = np.frombuffer(audio_data, dtype=np.int16)
            audio_data_sr = sr.AudioData(audio_data, self.sample_rate, 2)
            
            # GoogleéŸ³å£°èªè­˜ï¼ˆãƒãƒƒãƒˆæ¥ç¶šå¿…è¦ï¼‰
            try:
                text = self.recognizer.recognize_google(audio_data_sr, language='ja-JP')
                self.on_speech_recognized(text, len(audio_data))
            except sr.UnknownValueError:
                print("ğŸ”‡ Speech not recognized")
            except sr.RequestError as e:
                print(f"ğŸŒ Speech recognition service error: {e}")
                
        except Exception as e:
            print(f"Audio processing error: {e}")
    
    def on_speech_recognized(self, text: str, audio_length: int):
        """éŸ³å£°èªè­˜çµæœå‡¦ç†ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ç”¨ï¼‰"""
        print(f"ğŸ—£ï¸ Recognized: {text}")
    
    def stop_audio_stream(self):
        """éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ åœæ­¢"""
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.audio.terminate()
        print("âœ“ Audio stream stopped")

class VoiceControlMQTT:
    """éŸ³å£°åˆ¶å¾¡MQTTã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, broker_host: str = "localhost", device_id: str = None):
        self.broker_host = broker_host
        self.device_id = device_id or f"jetson_voice_{int(time.time())}"
        
        # éŸ³å£°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
        self.audio_processor = AudioProcessor()
        self.audio_processor.on_speech_recognized = self.on_speech_recognized
        
        # MQTT
        self.mqtt_client = mqtt.Client(client_id=self.device_id)
        self.setup_mqtt()
        
        # éŸ³å£°ã‚³ãƒãƒ³ãƒ‰è¾æ›¸
        self.voice_commands = {
            'ãƒ©ã‚¤ãƒˆã‚ªãƒ³': {'type': 'device_control', 'device': 'light', 'action': 'on'},
            'ãƒ©ã‚¤ãƒˆã‚ªãƒ•': {'type': 'device_control', 'device': 'light', 'action': 'off'},
            'ã‚¨ã‚¢ã‚³ãƒ³ã‚ªãƒ³': {'type': 'device_control', 'device': 'aircon', 'action': 'on'},
            'ã‚¨ã‚¢ã‚³ãƒ³ã‚ªãƒ•': {'type': 'device_control', 'device': 'aircon', 'action': 'off'},
            'ã‚¢ãƒ©ãƒ¼ãƒˆ': {'type': 'alert', 'level': 'warning', 'message': 'Voice alert triggered'},
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': {'type': 'status_request'},
            'ãƒ˜ãƒ«ãƒ—': {'type': 'help_request'}
        }
        
        print("ğŸ™ï¸ Voice Control MQTT initialized")
    
    def setup_mqtt(self):
        """MQTTè¨­å®š"""
        self.mqtt_client.on_connect = self.on_connect
        self.mqtt_client.on_disconnect = self.on_disconnect
    
    def on_connect(self, client, userdata, flags, rc):
        if rc == 0:
            print(f"âœ“ Voice Control MQTT connected: {self.device_id}")
        else:
            print(f"âœ— MQTT connection failed: {rc}")
    
    def on_disconnect(self, client, userdata, rc):
        if rc != 0:
            print(f"âš  Unexpected MQTT disconnect: {rc}")
    
    def on_speech_recognized(self, text: str, audio_length: int):
        """éŸ³å£°èªè­˜çµæœå‡¦ç†"""
        print(f"ğŸ—£ï¸ Voice input: {text}")
        
        # éŸ³å£°èªè­˜çµæœã‚’MQTTã§é€ä¿¡
        voice_data = {
            'timestamp': datetime.now().isoformat(),
            'device_id': self.device_id,
            'recognized_text': text,
            'audio_length_bytes': audio_length,
            'language': 'ja-JP'
        }
        
        topic = f"voice/{self.device_id}/recognition"
        self.mqtt_client.publish(topic, json.dumps(voice_data), qos=1)
        
        # ã‚³ãƒãƒ³ãƒ‰è§£æãƒ»å®Ÿè¡Œ
        command = self.parse_voice_command(text)
        if command:
            self.execute_voice_command(command)
    
    def parse_voice_command(self, text: str) -> Dict:
        """éŸ³å£°ã‚³ãƒãƒ³ãƒ‰è§£æ"""
        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        for keyword, command in self.voice_commands.items():
            if keyword in text:
                return command
        
        # éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        text_lower = text.lower()
        if 'ãƒ©ã‚¤ãƒˆ' in text and ('ã¤ã‘' in text or 'ã‚ªãƒ³' in text):
            return {'type': 'device_control', 'device': 'light', 'action': 'on'}
        elif 'ãƒ©ã‚¤ãƒˆ' in text and ('æ¶ˆã—' in text or 'ã‚ªãƒ•' in text):
            return {'type': 'device_control', 'device': 'light', 'action': 'off'}
        
        return None
    
    def execute_voice_command(self, command: Dict):
        """éŸ³å£°ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
        print(f"ğŸ¯ Executing voice command: {command}")
        
        command_data = {
            'timestamp': datetime.now().isoformat(),
            'device_id': self.device_id,
            'source': 'voice_recognition',
            'command': command
        }
        
        # ãƒ‡ãƒã‚¤ã‚¹åˆ¶å¾¡ã‚³ãƒãƒ³ãƒ‰
        if command['type'] == 'device_control':
            topic = f"control/device/{command['device']}/command"
            control_command = {
                'action': command['action'],
                'source': self.device_id,
                'timestamp': command_data['timestamp']
            }
            self.mqtt_client.publish(topic, json.dumps(control_command), qos=1)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆ
        elif command['type'] == 'alert':
            topic = f"alerts/{self.device_id}/voice"
            alert_data = {
                'level': command['level'],
                'message': command['message'],
                'source': 'voice_command',
                'timestamp': command_data['timestamp']
            }
            self.mqtt_client.publish(topic, json.dumps(alert_data), qos=1)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¦æ±‚
        elif command['type'] == 'status_request':
            topic = "system/status/request"
            self.mqtt_client.publish(topic, json.dumps(command_data), qos=1)
        
        # æ±ç”¨ã‚³ãƒãƒ³ãƒ‰ãƒ­ã‚°
        topic = f"voice/{self.device_id}/commands"
        self.mqtt_client.publish(topic, json.dumps(command_data), qos=1)
    
    async def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ¤ Starting Voice Control MQTT")
        
        # MQTTæ¥ç¶š
        try:
            self.mqtt_client.connect(self.broker_host, 1883, 60)
            self.mqtt_client.loop_start()
            
            # éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹
            if self.audio_processor.start_audio_stream():
                print("ğŸ‘‚ Listening for voice commands...")
                
                # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
                while True:
                    await asyncio.sleep(1)
            else:
                print("âŒ Failed to start audio stream")
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping voice control...")
        except Exception as e:
            print(f"Runtime error: {e}")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.audio_processor.stop_audio_stream()
        self.mqtt_client.loop_stop()
        self.mqtt_client.disconnect()
        print("âœ“ Voice control stopped")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import os
    
    broker_host = os.getenv('MQTT_BROKER_HOST', 'localhost')
    device_id = os.getenv('DEVICE_ID', None)
    
    voice_control = VoiceControlMQTT(broker_host, device_id)
    
    try:
        asyncio.run(voice_control.run())
    except KeyboardInterrupt:
        print("\nShutting down...")

if __name__ == "__main__":
    main()
```

### Step 3: çµ±åˆAIç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

#### 3.1 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼

`src/ai_orchestrator.py`:

```python
#!/usr/bin/env python3
import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import threading
import queue
from dataclasses import dataclass
from enum import Enum
import paho.mqtt.client as mqtt

from yolo_mqtt_detector import EdgeAIMQTTClient
from audio_recognition import VoiceControlMQTT

class AITaskType(Enum):
    OBJECT_DETECTION = "object_detection"
    VOICE_RECOGNITION = "voice_recognition"
    SENSOR_ANALYSIS = "sensor_analysis"
    BEHAVIOR_ANALYSIS = "behavior_analysis"

@dataclass
class AITask:
    task_id: str
    task_type: AITaskType
    priority: int  # 1=highest, 5=lowest
    data: Dict[str, Any]
    timestamp: float
    callback: Optional[callable] = None

class AIOrchestrator:
    """AIå‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, broker_host: str = "localhost", device_id: str = None):
        self.broker_host = broker_host
        self.device_id = device_id or f"jetson_orchestrator_{int(time.time())}"
        
        # ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ï¼ˆå„ªå…ˆåº¦ä»˜ãï¼‰
        self.task_queue = queue.PriorityQueue()
        self.active_tasks = {}
        self.completed_tasks = []
        
        # AIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        self.vision_client = None
        self.voice_client = None
        
        # MQTT
        self.mqtt_client = mqtt.Client(client_id=self.device_id)
        self.setup_mqtt()
        
        # çµ±è¨ˆ
        self.stats = {
            'tasks_processed': 0,
            'tasks_failed': 0,
            'average_processing_time': 0.0,
            'start_time': time.time()
        }
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰
        self.worker_threads = []
        self.running = False
        
        print("ğŸ§  AI Orchestrator initialized")
    
    def setup_mqtt(self):
        """MQTTè¨­å®š"""
        self.mqtt_client.on_connect = self.on_connect
        self.mqtt_client.on_message = self.on_message
    
    def on_connect(self, client, userdata, flags, rc):
        if rc == 0:
            print(f"âœ“ AI Orchestrator MQTT connected: {self.device_id}")
            
            # AIã‚¿ã‚¹ã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆè³¼èª­
            client.subscribe(f"ai/{self.device_id}/task/request", qos=1)
            client.subscribe("ai/broadcast/task/request", qos=1)
            
            # ä»–ã®AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®çµæœè³¼èª­
            client.subscribe("ai/+/detections", qos=1)
            client.subscribe("voice/+/recognition", qos=1)
            client.subscribe("sensors/+/+/data", qos=1)
    
    def on_message(self, client, userdata, msg):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡å‡¦ç†"""
        try:
            topic_parts = msg.topic.split('/')
            payload = json.loads(msg.payload.decode())
            
            if "/task/request" in msg.topic:
                self.handle_task_request(payload)
            elif "/detections" in msg.topic:
                self.handle_detection_result(payload)
            elif "/recognition" in msg.topic:
                self.handle_voice_result(payload)
            elif "/data" in msg.topic:
                self.handle_sensor_data(payload)
                
        except Exception as e:
            print(f"Message processing error: {e}")
    
    def handle_task_request(self, request: Dict):
        """AIã‚¿ã‚¹ã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†"""
        task_type_str = request.get('task_type')
        priority = request.get('priority', 3)
        task_data = request.get('data', {})
        
        try:
            task_type = AITaskType(task_type_str)
            task_id = f"{task_type_str}_{int(time.time() * 1000)}"
            
            task = AITask(
                task_id=task_id,
                task_type=task_type,
                priority=priority,
                data=task_data,
                timestamp=time.time()
            )
            
            self.submit_task(task)
            
        except ValueError:
            print(f"Unknown task type: {task_type_str}")
    
    def handle_detection_result(self, result: Dict):
        """ç‰©ä½“æ¤œå‡ºçµæœå‡¦ç†"""
        # è¡Œå‹•è§£æã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
        if result.get('detections'):
            behavior_task = AITask(
                task_id=f"behavior_{int(time.time() * 1000)}",
                task_type=AITaskType.BEHAVIOR_ANALYSIS,
                priority=2,
                data={'detection_result': result},
                timestamp=time.time()
            )
            self.submit_task(behavior_task)
    
    def handle_voice_result(self, result: Dict):
        """éŸ³å£°èªè­˜çµæœå‡¦ç†"""
        # éŸ³å£°ã‚³ãƒãƒ³ãƒ‰ã®æ„å›³åˆ†æ
        text = result.get('recognized_text', '')
        if text:
            print(f"ğŸ¤ Processing voice: {text}")
            # ã‚ˆã‚Šé«˜åº¦ãªè‡ªç„¶è¨€èªå‡¦ç†ã‚’ã“ã“ã«å®Ÿè£…
    
    def handle_sensor_data(self, data: Dict):
        """ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ç•°å¸¸æ¤œå‡º
        sensors = data.get('sensors', {})
        
        # ç•°å¸¸å€¤æ¤œå‡ºã®ç°¡å˜ãªä¾‹
        for sensor_name, sensor_data in sensors.items():
            temp = sensor_data.get('temperature_c')
            if temp and (temp > 40 or temp < 0):  # ç•°å¸¸æ¸©åº¦
                analysis_task = AITask(
                    task_id=f"sensor_analysis_{int(time.time() * 1000)}",
                    task_type=AITaskType.SENSOR_ANALYSIS,
                    priority=1,  # é«˜å„ªå…ˆåº¦
                    data={'sensor_anomaly': {'sensor': sensor_name, 'value': temp}},
                    timestamp=time.time()
                )
                self.submit_task(analysis_task)
    
    def submit_task(self, task: AITask):
        """ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        # å„ªå…ˆåº¦ä»˜ãã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆæ•°å€¤ãŒå°ã•ã„ã»ã©é«˜å„ªå…ˆåº¦ï¼‰
        self.task_queue.put((task.priority, task.timestamp, task))
        print(f"ğŸ“‹ Task submitted: {task.task_id} (priority: {task.priority})")
    
    async def initialize_ai_modules(self):
        """AIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–"""
        try:
            # ç‰©ä½“æ¤œå‡ºã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.vision_client = EdgeAIMQTTClient(self.broker_host, f"{self.device_id}_vision")
            
            # éŸ³å£°èªè­˜ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.voice_client = VoiceControlMQTT(self.broker_host, f"{self.device_id}_voice")
            
            print("âœ“ AI modules initialized")
            
        except Exception as e:
            print(f"AI module initialization error: {e}")
    
    def start_workers(self, num_workers: int = 2):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹"""
        self.running = True
        
        for i in range(num_workers):
            worker = threading.Thread(target=self.worker_loop, args=(i,), daemon=True)
            worker.start()
            self.worker_threads.append(worker)
        
        print(f"âœ“ Started {num_workers} worker threads")
    
    def worker_loop(self, worker_id: int):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ«ãƒ¼ãƒ—"""
        print(f"ğŸ‘· Worker {worker_id} started")
        
        while self.running:
            try:
                # ã‚¿ã‚¹ã‚¯å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                priority, timestamp, task = self.task_queue.get(timeout=1.0)
                
                # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
                start_time = time.time()
                result = self.execute_task(task)
                processing_time = time.time() - start_time
                
                # çµæœå‡¦ç†
                self.handle_task_result(task, result, processing_time)
                
                # çµ±è¨ˆæ›´æ–°
                self.update_stats(processing_time, True)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
                self.update_stats(0, False)
    
    def execute_task(self, task: AITask) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        print(f"âš¡ Executing task: {task.task_id} (type: {task.task_type.value})")
        
        if task.task_type == AITaskType.BEHAVIOR_ANALYSIS:
            return self.analyze_behavior(task.data)
        elif task.task_type == AITaskType.SENSOR_ANALYSIS:
            return self.analyze_sensor_anomaly(task.data)
        else:
            return {'status': 'not_implemented', 'task_type': task.task_type.value}
    
    def analyze_behavior(self, data: Dict) -> Dict[str, Any]:
        """è¡Œå‹•è§£æï¼ˆç°¡å˜ãªä¾‹ï¼‰"""
        detection_result = data.get('detection_result', {})
        detections = detection_result.get('detections', [])
        
        behaviors = []
        
        # äººã®æ¤œå‡ºæ•°ã«ã‚ˆã‚‹åˆ†æ
        person_count = sum(1 for d in detections if d.get('class_name') == 'person')
        if person_count > 5:
            behaviors.append({
                'type': 'crowding',
                'severity': 'medium',
                'description': f'Many people detected: {person_count}'
            })
        
        # è»Šä¸¡æ¤œå‡ºã«ã‚ˆã‚‹åˆ†æ
        vehicle_classes = ['car', 'truck', 'bus', 'motorcycle']
        vehicle_count = sum(1 for d in detections if d.get('class_name') in vehicle_classes)
        if vehicle_count > 3:
            behaviors.append({
                'type': 'traffic_congestion',
                'severity': 'low',
                'description': f'Multiple vehicles detected: {vehicle_count}'
            })
        
        return {
            'status': 'completed',
            'behaviors': behaviors,
            'person_count': person_count,
            'vehicle_count': vehicle_count
        }
    
    def analyze_sensor_anomaly(self, data: Dict) -> Dict[str, Any]:
        """ã‚»ãƒ³ã‚µãƒ¼ç•°å¸¸åˆ†æ"""
        anomaly = data.get('sensor_anomaly', {})
        sensor_name = anomaly.get('sensor')
        value = anomaly.get('value')
        
        analysis_result = {
            'status': 'completed',
            'anomaly_detected': True,
            'sensor': sensor_name,
            'value': value,
            'recommendations': []
        }
        
        # æ¸©åº¦ç•°å¸¸ã®å ´åˆ
        if 'temperature' in sensor_name:
            if value > 40:
                analysis_result['severity'] = 'high'
                analysis_result['recommendations'].append('Check for fire or overheating')
                analysis_result['recommendations'].append('Activate cooling systems')
            elif value < 0:
                analysis_result['severity'] = 'medium'
                analysis_result['recommendations'].append('Check for freezing conditions')
                analysis_result['recommendations'].append('Activate heating systems')
        
        return analysis_result
    
    def handle_task_result(self, task: AITask, result: Dict, processing_time: float):
        """ã‚¿ã‚¹ã‚¯çµæœå‡¦ç†"""
        print(f"âœ… Task completed: {task.task_id} ({processing_time:.2f}s)")
        
        # çµæœã‚’MQTTé€ä¿¡
        result_data = {
            'timestamp': datetime.now().isoformat(),
            'device_id': self.device_id,
            'task_id': task.task_id,
            'task_type': task.task_type.value,
            'processing_time_seconds': round(processing_time, 3),
            'result': result
        }
        
        topic = f"ai/{self.device_id}/task/result"
        self.mqtt_client.publish(topic, json.dumps(result_data), qos=1)
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
        if task.callback:
            try:
                task.callback(result)
            except Exception as e:
                print(f"Callback error: {e}")
    
    def update_stats(self, processing_time: float, success: bool):
        """çµ±è¨ˆæ›´æ–°"""
        if success:
            self.stats['tasks_processed'] += 1
            # ç§»å‹•å¹³å‡ã§ã®å‡¦ç†æ™‚é–“æ›´æ–°
            if self.stats['average_processing_time'] == 0:
                self.stats['average_processing_time'] = processing_time
            else:
                alpha = 0.1  # é‡ã¿
                self.stats['average_processing_time'] = (
                    alpha * processing_time + 
                    (1 - alpha) * self.stats['average_processing_time']
                )
        else:
            self.stats['tasks_failed'] += 1
    
    def publish_status(self):
        """çŠ¶æ…‹é€ä¿¡"""
        uptime = time.time() - self.stats['start_time']
        
        status_data = {
            'timestamp': datetime.now().isoformat(),
            'device_id': self.device_id,
            'status': 'running',
            'uptime_seconds': round(uptime, 2),
            'statistics': self.stats,
            'queue_size': self.task_queue.qsize(),
            'active_workers': len(self.worker_threads)
        }
        
        topic = f"ai/{self.device_id}/orchestrator/status"
        self.mqtt_client.publish(topic, json.dumps(status_data), qos=1, retain=True)
    
    async def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ§  Starting AI Orchestrator")
        
        # MQTTæ¥ç¶š
        try:
            self.mqtt_client.connect(self.broker_host, 1883, 60)
            self.mqtt_client.loop_start()
            
            # AIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–
            await self.initialize_ai_modules()
            
            # ãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹
            self.start_workers(num_workers=3)
            
            # å®šæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡
            last_status_time = 0
            
            while True:
                current_time = time.time()
                
                # 5åˆ†é–“éš”ã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡
                if current_time - last_status_time >= 300:
                    self.publish_status()
                    last_status_time = current_time
                
                await asyncio.sleep(10)
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping AI Orchestrator...")
        except Exception as e:
            print(f"Runtime error: {e}")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.running = False
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢å¾…æ©Ÿ
        for worker in self.worker_threads:
            worker.join(timeout=2)
        
        # MQTTåˆ‡æ–­
        self.mqtt_client.loop_stop()
        self.mqtt_client.disconnect()
        
        print("âœ“ AI Orchestrator stopped")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import os
    
    broker_host = os.getenv('MQTT_BROKER_HOST', 'localhost')
    device_id = os.getenv('DEVICE_ID', None)
    
    orchestrator = AIOrchestrator(broker_host, device_id)
    
    try:
        asyncio.run(orchestrator.run())
    except KeyboardInterrupt:
        print("\nShutting down...")

if __name__ == "__main__":
    main()
```

## èª²é¡Œ

### åŸºç¤èª²é¡Œ

1. **YOLOç‰©ä½“æ¤œå‡ºå®Ÿè£…**
   - ã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“æ¤œå‡º
   - æ¤œå‡ºçµæœã®MQTTé€ä¿¡

2. **éŸ³å£°èªè­˜çµ±åˆ**
   - æ—¥æœ¬èªéŸ³å£°èªè­˜
   - éŸ³å£°ã‚³ãƒãƒ³ãƒ‰ã®MQTTåˆ¶å¾¡

3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
   - TensorRTæœ€é©åŒ–
   - FPSå‘ä¸Š

### å¿œç”¨èª²é¡Œ

1. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI**
   - ç”»åƒãƒ»éŸ³å£°ãƒ»ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿çµ±åˆ
   - è¤‡åˆçš„ãªçŠ¶æ³åˆ¤æ–­

2. **ã‚¨ãƒƒã‚¸-ã‚¯ãƒ©ã‚¦ãƒ‰é€£æº**
   - ã‚¨ãƒƒã‚¸å‰å‡¦ç†ã€ã‚¯ãƒ©ã‚¦ãƒ‰è©³ç´°åˆ†æ
   - é©å¿œçš„è² è·åˆ†æ•£

3. **ã‚«ã‚¹ã‚¿ãƒ AIãƒ¢ãƒ‡ãƒ«**
   - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’
   - ã‚¨ãƒƒã‚¸ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### GPUãƒ»CUDAé–¢é€£
```bash
# CUDAç¢ºèª
nvidia-smi
nvcc --version

# PyTorchã®CUDAå¯¾å¿œç¢ºèª
python3 -c "import torch; print(torch.cuda.is_available())"

# TensorRTç¢ºèª
python3 -c "import tensorrt; print(tensorrt.__version__)"
```

### ã‚«ãƒ¡ãƒ©é–¢é€£
```bash
# CSIã‚«ãƒ¡ãƒ©ç¢ºèª
ls /dev/video*

# USBã‚«ãƒ¡ãƒ©ãƒ†ã‚¹ãƒˆ
v4l2-ctl --list-devices
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

æ¼”ç¿’5ã§è¤‡æ•°ãƒ‡ãƒã‚¤ã‚¹ã‚’çµ±åˆã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªIoTã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

---

ã“ã®ã‚¨ãƒƒã‚¸AIçµ±åˆã«ã‚ˆã‚Šã€Jetsonã®çœŸã®èƒ½åŠ›ã‚’æ´»ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ AIã‚·ã‚¹ãƒ†ãƒ ã‚’ä½“é¨“ã§ãã¾ã™ã€‚